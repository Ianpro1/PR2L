{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianmi\\miniconda3\\envs\\GPUtorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import ptan\n",
    "import common\n",
    "import atari_wrappers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ptan_copy\n",
    "\n",
    "\n",
    "\n",
    "EPSILON = 1.0\n",
    "GAMMA = 0.99\n",
    "REPLAY_SIZE = 2000\n",
    "TGT_NET_SYNC = 10\n",
    "EPS_DECAY = 0.96\n",
    "BATCH_SIZE = 800\n",
    "\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "env = atari_wrappers.reshapeWrapper(env)\n",
    "env = atari_wrappers.ScaledFloatFrame(env)\n",
    "env = atari_wrappers.oldWrapper(env)\n",
    "env = atari_wrappers.MaxAndSkipEnv(env)\n",
    "env = atari_wrappers.DumbRewardWrapper(env)\n",
    "\n",
    "\n",
    "PATH = \"Breakout-v4.pt\"\n",
    "device = \"cpu\"\n",
    "\n",
    "net = common.DQN((3, 210,160), env.action_space.n).to(device)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "preprocessor = common.ndarray_preprocessor(common.VariableTensor_preprocessor())\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=EPSILON)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector=selector, device=device, preprocessor=preprocessor)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.0)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m buffer\u001b[39m.\u001b[39mpopulate(\u001b[39m3\u001b[39m)\n\u001b[0;32m     21\u001b[0m batch \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39msample(\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m states_v, actions_v, tgt_states_v \u001b[39m=\u001b[39m unpack_batch(batch) \u001b[39m# *elements are state, action, reward, last_state \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#(verify that the reward is truly immediate by verifying from the function __iter__ that states1 == returned_states but returned r from r1 and r2 discounted is = r1)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#also look for GAMMA effect (default should be 1 for desired no discounting)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "exp_source2 = ptan_copy.experience.ExperienceSourceFirstLast(env, agent,gamma=0.5)\n",
    "\n",
    "idx = 0\n",
    "done_id = 0\n",
    "episode = 0\n",
    "def unpack_batch(batch): # return states, actions, calculated tgt_q_v = r + tgt_net(last_state)*GAMMA\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        rewards.append(exp.reward)\n",
    "        actions.append(exp.action)\n",
    "        last_states.append(exp.last_state)\n",
    "    print(len(states), rewards)\n",
    "    \n",
    "\n",
    "while True:\n",
    "    buffer.populate(3)\n",
    "    batch = buffer.sample(3)\n",
    "    states_v, actions_v, tgt_states_v = unpack_batch(batch) # *elements are state, action, reward, last_state \n",
    "    \n",
    "\n",
    "\n",
    "    #(verify that the reward is truly immediate by verifying from the function __iter__ that states1 == returned_states but returned r from r1 and r2 discounted is = r1)\n",
    "    #also look for GAMMA effect (default should be 1 for desired no discounting)\n",
    "    break\n",
    "\n",
    "\n",
    "    '''\n",
    "    for rewards, steps in exp_source.pop_rewards_steps():\n",
    "        episode +=1\n",
    "        print(\"idx %d, steps %d, episode %d done, reward=%.3f rewards, epsilon=%.2f\" %(idx,steps,episode, rewards, selector.epsilon))\n",
    "        solved = rewards > 150\n",
    "    if solved:\n",
    "        print(\"Done in %d idx\" % idx)\n",
    "        break\n",
    "    if len(buffer) < 2*BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    states_v, actions_v, tgt_q_v = unpack_batch(\n",
    "    batch, tgt_net.target_model, GAMMA)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(states_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = torch.nn.functional.mse_loss(q_v, tgt_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    selector.epsilon *= EPS_DECAY\n",
    "\n",
    "    if step % TGT_NET_SYNC == 0:\n",
    "        tgt_net.sync()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5aaa6110c051b252f9bc5cac8ee3d9ddd5264f49c7610fe0e84bda82603ad25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
